{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51dda7c9",
        "outputId": "32ebc240-21f2-4e40-8dcd-dd7e566fd70f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.48.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.1)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m805.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.0.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.26.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.48.0-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.48.0 watchdog-6.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### App.py code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc3f6cfd",
        "outputId": "4520fb14-ee58-4a27-8354-40a8e85f61d2"
      },
      "outputs": [],
      "source": [
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import xgboost as xgb\n",
        "import logging\n",
        "from typing import List, Dict, Tuple\n",
        "import itertools\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Define the XGBoostCareerRecommender class \n",
        "class XGBoostCareerRecommender:\n",
        "    def __init__(self, n_estimators=100, max_depth=6, learning_rate=0.1,\n",
        "                 objective='reg:squarederror', random_state=42):\n",
        "        \"\"\"\n",
        "        Initialize the XGBoost Career Recommender\n",
        "\n",
        "        Args:\n",
        "            n_estimators: Number of boosting rounds\n",
        "            max_depth: Maximum depth of trees\n",
        "            learning_rate: Learning rate\n",
        "            objective: XGBoost objective function\n",
        "            random_state: Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.learning_rate = learning_rate\n",
        "        self.objective = objective\n",
        "        self.random_state = random_state\n",
        "\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.course_encoder = LabelEncoder()\n",
        "        self.feature_names = []\n",
        "\n",
        "    def extract_advanced_features(self, students_df: pd.DataFrame, courses_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Extract comprehensive features for XGBoost training\n",
        "\n",
        "        Args:\n",
        "            students_df: DataFrame with student information\n",
        "            courses_df: DataFrame with course information\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with engineered features\n",
        "        \"\"\"\n",
        "        features_list = []\n",
        "\n",
        "        # Define subject categories\n",
        "        all_subjects = ['Mathematics', 'Physics', 'Chemistry', 'Biology', 'English', 'Geography', 'History']\n",
        "        stem_subjects = ['Mathematics', 'Physics', 'Chemistry', 'Biology']\n",
        "        humanities_subjects = ['English', 'Geography', 'History']\n",
        "\n",
        "        # Define interest categories\n",
        "        interest_categories = {\n",
        "            'technology': ['programming', 'computers', 'innovation', 'AI', 'software', 'data', 'machine learning', 'robotics'],\n",
        "            'healthcare': ['medicine', 'nursing', 'biology', 'helping', 'health', 'medical', 'care'],\n",
        "            'business': ['entrepreneurship', 'marketing', 'finance', 'management', 'economics', 'business'],\n",
        "            'creative': ['art', 'design', 'music', 'writing', 'creative', 'media', 'storytelling'],\n",
        "            'social': ['teaching', 'counseling', 'social work', 'psychology', 'education', 'helping people'],\n",
        "            'research': ['research', 'analysis', 'investigation', 'study', 'academic'],\n",
        "            'communication': ['communication', 'public speaking', 'presentation', 'media', 'journalism']\n",
        "        }\n",
        "\n",
        "        for index in students_df.index:\n",
        "            student = students_df.loc[index]\n",
        "\n",
        "            if 'subjects' not in student:\n",
        "                logger.error(f\"Error: 'subjects' not in student data for index {index}. Columns: {student.index.tolist()}\")\n",
        "                continue # Skip this student if 'subjects' is missing\n",
        "\n",
        "            student_subjects = set(student['subjects'].split(', '))\n",
        "            student_interests = set(student['interests'].split(', '))\n",
        "\n",
        "            for _, course in courses_df.iterrows():\n",
        "                course_subjects = set(course['subjects'].split(', '))\n",
        "                course_skills = set(course['skills'].split(', '))\n",
        "\n",
        "                features = {}\n",
        "\n",
        "                # Student ID and Course ID (for reference)\n",
        "                features['student_id'] = student['student_id']\n",
        "                features['course_name'] = course['course_name']\n",
        "\n",
        "                # Basic subject matching features\n",
        "                for subject in all_subjects:\n",
        "                    features[f'student_has_{subject.lower()}'] = 1 if subject in student_subjects else 0\n",
        "                    features[f'course_requires_{subject.lower()}'] = 1 if subject in course_subjects else 0\n",
        "\n",
        "                # Subject overlap features\n",
        "                features['subject_overlap_count'] = len(student_subjects.intersection(course_subjects))\n",
        "                features['subject_overlap_ratio'] = (len(student_subjects.intersection(course_subjects)) /\n",
        "                                                   len(student_subjects.union(course_subjects)) if student_subjects.union(course_subjects) else 0)\n",
        "\n",
        "                # Interest-skill alignment features\n",
        "                features['interest_skill_overlap'] = len(student_interests.intersection(course_skills))\n",
        "                features['interest_skill_ratio'] = (len(student_interests.intersection(course_skills)) /\n",
        "                                                   len(student_interests.union(course_skills)) if student_interests.union(course_skills) else 0)\n",
        "\n",
        "                # Subject category features\n",
        "                student_stem_count = sum(1 for subj in student_subjects if subj in stem_subjects)\n",
        "                student_humanities_count = sum(1 for subj in student_subjects if subj in humanities_subjects)\n",
        "\n",
        "                course_stem_count = sum(1 for subj in course_subjects if subj in stem_subjects)\n",
        "                course_humanities_count = sum(1 for subj in course_subjects if subj in humanities_subjects)\n",
        "\n",
        "                features['student_stem_ratio'] = student_stem_count / len(stem_subjects)\n",
        "                features['student_humanities_ratio'] = student_humanities_count / len(humanities_subjects)\n",
        "                features['course_stem_ratio'] = course_stem_count / len(stem_subjects)\n",
        "                features['course_humanities_ratio'] = course_humanities_count / len(humanities_subjects)\n",
        "\n",
        "                # Alignment between student and course preferences\n",
        "                features['stem_alignment'] = min(features['student_stem_ratio'], features['course_stem_ratio'])\n",
        "                features['humanities_alignment'] = min(features['student_humanities_ratio'], features['course_humanities_ratio'])\n",
        "\n",
        "                # Interest category features\n",
        "                for category, keywords in interest_categories.items():\n",
        "                    student_category_score = sum(1 for interest in student_interests\n",
        "                                               if any(keyword.lower() in interest.lower() for keyword in keywords))\n",
        "                    course_category_score = sum(1 for skill in course_skills\n",
        "                                              if any(keyword.lower() in skill.lower() for keyword in keywords))\n",
        "\n",
        "                    features[f'student_{category}_interest'] = student_category_score\n",
        "                    features[f'course_{category}_relevance'] = course_category_score\n",
        "                    features[f'{category}_alignment'] = min(student_category_score, course_category_score)\n",
        "\n",
        "                # Diversity features\n",
        "                features['student_subject_diversity'] = len(student_subjects)\n",
        "                features['student_interest_diversity'] = len(student_interests)\n",
        "                features['course_subject_breadth'] = len(course_subjects)\n",
        "                features['course_skill_breadth'] = len(course_skills)\n",
        "\n",
        "                # Target variable: compatibility score\n",
        "                subject_score = len(student_subjects.intersection(course_subjects))\n",
        "                interest_score = len(student_interests.intersection(course_skills))\n",
        "                total_possible = len(student_subjects) + len(student_interests)\n",
        "\n",
        "                features['compatibility_score'] = (subject_score + interest_score) / total_possible if total_possible > 0 else 0\n",
        "\n",
        "                features_list.append(features)\n",
        "\n",
        "        return pd.DataFrame(features_list)\n",
        "\n",
        "\n",
        "    def prepare_training_data(self, students_df: pd.DataFrame, courses_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Prepare training data for XGBoost\n",
        "\n",
        "        Args:\n",
        "            students_df: DataFrame with student information\n",
        "            courses_df: DataFrame with course information\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (features, targets)\n",
        "        \"\"\"\n",
        "        logger.info(\"Extracting advanced features...\")\n",
        "\n",
        "        # Extract features\n",
        "        features_df = self.extract_advanced_features(students_df, courses_df)\n",
        "\n",
        "        # Separate features and target\n",
        "        target_col = 'compatibility_score'\n",
        "        feature_cols = [col for col in features_df.columns\n",
        "                       if col not in ['student_id', 'course_name', target_col]]\n",
        "\n",
        "        X = features_df[feature_cols]\n",
        "        y = features_df[target_col]\n",
        "\n",
        "        # Store feature names\n",
        "        self.feature_names = feature_cols\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        logger.info(f\"Prepared {X_scaled.shape[0]} training samples with {X_scaled.shape[1]} features\")\n",
        "\n",
        "        return X_scaled, y.values\n",
        "\n",
        "    def train(self, students_df: pd.DataFrame, courses_df: pd.DataFrame,\n",
        "              test_size=0.2, early_stopping_rounds=20):\n",
        "        \"\"\"\n",
        "        Train the XGBoost model\n",
        "\n",
        "        Args:\n",
        "            students_df: DataFrame with student information\n",
        "            courses_df: DataFrame with course information\n",
        "            test_size: Fraction of data for testing\n",
        "            early_stopping_rounds: Early stopping patience\n",
        "        \"\"\"\n",
        "        logger.info(\"Preparing training data...\")\n",
        "\n",
        "        # Prepare training data\n",
        "        X, y = self.prepare_training_data(students_df, courses_df)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=self.random_state, stratify=None\n",
        "        )\n",
        "\n",
        "        # Initialize XGBoost model\n",
        "        self.model = xgb.XGBRegressor(\n",
        "            n_estimators=self.n_estimators,\n",
        "            max_depth=self.max_depth,\n",
        "            learning_rate=self.learning_rate,\n",
        "            objective=self.objective,\n",
        "            random_state=self.random_state,\n",
        "            reg_alpha=0.1,  # L1 regularization\n",
        "            reg_lambda=0.1,  # L2 regularization\n",
        "            subsample=0.8,   # Subsample ratio\n",
        "            colsample_bytree=0.8,  # Feature sampling\n",
        "            eval_metric='rmse'\n",
        "        )\n",
        "\n",
        "        # Train model without early stopping callbacks to resolve TypeError\n",
        "        logger.info(\"Training XGBoost model...\")\n",
        "\n",
        "        self.model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_train, y_train), (X_test, y_test)],\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        # Evaluate model\n",
        "        train_pred = self.model.predict(X_train)\n",
        "        test_pred = self.model.predict(X_test)\n",
        "\n",
        "        train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
        "        test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
        "\n",
        "        logger.info(f\"Training RMSE: {train_rmse:.4f}\")\n",
        "        logger.info(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "\n",
        "        # Feature importance\n",
        "        feature_importance = self.model.feature_importances_\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': self.feature_names,\n",
        "            'importance': feature_importance\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        logger.info(\"Top 10 most important features:\")\n",
        "        for _, row in importance_df.head(10).iterrows():\n",
        "            logger.info(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "        logger.info(\"Training completed!\")\n",
        "\n",
        "        return {\n",
        "            'train_rmse': train_rmse,\n",
        "            'test_rmse': test_rmse,\n",
        "            'feature_importance': importance_df\n",
        "        }\n",
        "\n",
        "    def predict_for_student(self, student_data: Dict, courses_df: pd.DataFrame, top_k=5) -> List[Tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        Predict career recommendations for a single student\n",
        "\n",
        "        Args:\n",
        "            student_data: Dictionary with 'subjects' and 'interests' keys\n",
        "            courses_df: DataFrame with course information\n",
        "            top_k: Number of top recommendations to return\n",
        "        Returns:\n",
        "            List of (course_name, confidence_score) tuples\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "\n",
        "        # Create a temporary DataFrame for the student\n",
        "        temp_student_df = pd.DataFrame([{\n",
        "            'student_id': 'temp_student',\n",
        "            'subjects': ', '.join(student_data['subjects']),\n",
        "            'interests': ', '.join(student_data['interests'])\n",
        "        }])\n",
        "\n",
        "        # Extract features for all student-course combinations\n",
        "        features_df = self.extract_advanced_features(temp_student_df, courses_df)\n",
        "\n",
        "        # Prepare features for prediction\n",
        "        feature_cols = [col for col in features_df.columns\n",
        "                       if col not in ['student_id', 'course_name', 'compatibility_score']]\n",
        "\n",
        "        X = features_df[feature_cols]\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = self.model.predict(X_scaled)\n",
        "\n",
        "        # Combine with course names\n",
        "        recommendations = []\n",
        "        for i, course_name in enumerate(features_df['course_name']):\n",
        "            recommendations.append((course_name, float(predictions[i])))\n",
        "\n",
        "        # Sort by prediction score and return top K\n",
        "        recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return recommendations[:top_k]\n",
        "\n",
        "    def get_feature_importance(self, top_n=20) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Get feature importance from trained model\n",
        "\n",
        "        Args:\n",
        "            top_n: Number of top features to return\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with feature importance\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': self.feature_names,\n",
        "            'importance': self.model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        return importance_df.head(top_n)\n",
        "\n",
        "    def save_model(self, filepath: str):\n",
        "        \"\"\"Save the trained model and preprocessors\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"No model to save!\")\n",
        "\n",
        "        # Save XGBoost model\n",
        "        self.model.save_model(f\"{filepath}_xgboost.json\")\n",
        "\n",
        "        # Save preprocessors and metadata\n",
        "        with open(f\"{filepath}_preprocessors.pkl\", 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'scaler': self.scaler,\n",
        "                'course_encoder': self.course_encoder,\n",
        "                'feature_names': self.feature_names,\n",
        "                'n_estimators': self.n_estimators,\n",
        "                'max_depth': self.max_depth,\n",
        "                'learning_rate': self.learning_rate,\n",
        "                'objective': self.objective,\n",
        "                'random_state': self.random_state\n",
        "            }, f)\n",
        "\n",
        "        logger.info(f\"Model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath: str):\n",
        "        \"\"\"Load a trained model and preprocessors\"\"\"\n",
        "        # Load preprocessors and metadata first\n",
        "        with open(f\"{filepath}_preprocessors.pkl\", 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            self.scaler = data['scaler']\n",
        "            self.course_encoder = data['course_encoder']\n",
        "            self.feature_names = data['feature_names']\n",
        "            self.n_estimators = data['n_estimators']\n",
        "            self.max_depth = data['max_depth']\n",
        "            self.learning_rate = data['learning_rate']\n",
        "            self.objective = data['objective']\n",
        "            self.random_state = data['random_state']\n",
        "\n",
        "        # Initialize the model with loaded hyperparameters before loading the model state\n",
        "        self.model = xgb.XGBRegressor(\n",
        "            n_estimators=self.n_estimators,\n",
        "            max_depth=self.max_depth,\n",
        "            learning_rate=self.learning_rate,\n",
        "            objective=self.objective,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "        # Load XGBoost model state\n",
        "        self.model.load_model(f\"{filepath}_xgboost.json\")\n",
        "\n",
        "\n",
        "        logger.info(f\"Model loaded from {filepath}\")\n",
        "\n",
        "## Where the magic happens\n",
        "# --- Streamlit App Code ---\n",
        "\n",
        "st.title(\"Career Recommendation System\")\n",
        "\n",
        "# Load the trained model and data\n",
        "@st.cache_resource\n",
        "def load_recommender_and_data():\n",
        "    recommender = XGBoostCareerRecommender()\n",
        "    try:\n",
        "        recommender.load_model('xgboost_career_model')\n",
        "        courses_df = pd.read_csv('./Courses.csv')\n",
        "        return recommender, courses_df\n",
        "    except FileNotFoundError:\n",
        "        st.error(\"Model files not found. Please run the training code first.\")\n",
        "        return None, None\n",
        "\n",
        "recommender, courses_df = load_recommender_and_data()\n",
        "\n",
        "if recommender and courses_df is not None:\n",
        "    st.write(\"Enter your academic subjects and interests to get career recommendations.\")\n",
        "\n",
        "    # Get user input\n",
        "    subjects_input = st.text_input(\"Enter your subjects (comma-separated, e.g., Mathematics, Physics, English)\")\n",
        "    interests_input = st.text_input(\"Enter your interests (comma-separated, e.g., Art, Business, AI, Data Science)\")\n",
        "\n",
        "    if st.button(\"Get Recommendations\"):\n",
        "        if subjects_input and interests_input:\n",
        "            student_subjects = [s.strip() for s in subjects_input.split(',')]\n",
        "            student_interests = [i.strip() for i in interests_input.split(',')]\n",
        "\n",
        "            student_data = {\n",
        "                'subjects': student_subjects,\n",
        "                'interests': student_interests\n",
        "            }\n",
        "\n",
        "            with st.spinner(\"Getting recommendations...\"):\n",
        "                recommendations = recommender.predict_for_student(student_data, courses_df)\n",
        "\n",
        "            st.subheader(\"Top Career Recommendations:\")\n",
        "            if recommendations:\n",
        "                for course, confidence in recommendations:\n",
        "                    st.write(f\"- {course} (Confidence: {confidence:.3f})\")\n",
        "            else:\n",
        "                st.info(\"No recommendations found based on your input.\")\n",
        "        else:\n",
        "            st.warning(\"Please enter both subjects and interests.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc2ec10c",
        "outputId": "ea2c7446-fdc1-481a-9ee3-84e53cc00125"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.234.6.87:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K\u001b[1G\u001b[0JNeed to install the following packages:\n",
            "localtunnel@2.0.2\n",
            "Ok to proceed? (y) \u001b[20Gy\n",
            "\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0K⠋\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0Kyour url is: https://fluffy-kings-eat.loca.lt\n",
            "INFO:__main__:Model loaded from xgboost_career_model\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
