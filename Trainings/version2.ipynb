{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, mean_squared_error\n",
        "import pickle\n",
        "import logging\n",
        "from typing import List, Dict, Tuple\n",
        "import itertools\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class XGBoostCareerRecommender:\n",
        "    def __init__(self, n_estimators=100, max_depth=6, learning_rate=0.1,\n",
        "                 objective='reg:squarederror', random_state=42):\n",
        "        \"\"\"\n",
        "        Initialize the XGBoost Career Recommender\n",
        "\n",
        "        Args:\n",
        "            n_estimators: Number of boosting rounds\n",
        "            max_depth: Maximum depth of trees\n",
        "            learning_rate: Learning rate\n",
        "            objective: XGBoost objective function\n",
        "            random_state: Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.learning_rate = learning_rate\n",
        "        self.objective = objective\n",
        "        self.random_state = random_state\n",
        "\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.course_encoder = LabelEncoder()\n",
        "        self.feature_names = []\n",
        "\n",
        "    def extract_advanced_features(self, students_df: pd.DataFrame, courses_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Extract comprehensive features for XGBoost training\n",
        "\n",
        "        Args:\n",
        "            students_df: DataFrame with student information\n",
        "            courses_df: DataFrame with course information\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with engineered features\n",
        "        \"\"\"\n",
        "        features_list = []\n",
        "\n",
        "        # Define subject categories\n",
        "        all_subjects = ['Mathematics', 'Physics', 'Chemistry', 'Biology', 'English', 'Geography', 'History']\n",
        "        stem_subjects = ['Mathematics', 'Physics', 'Chemistry', 'Biology']\n",
        "        humanities_subjects = ['English', 'Geography', 'History']\n",
        "\n",
        "        # Define interest categories\n",
        "        interest_categories = {\n",
        "            'technology': ['programming', 'computers', 'innovation', 'AI', 'software', 'data', 'machine learning', 'robotics'],\n",
        "            'healthcare': ['medicine', 'nursing', 'biology', 'helping', 'health', 'medical', 'care'],\n",
        "            'business': ['entrepreneurship', 'marketing', 'finance', 'management', 'economics', 'business'],\n",
        "            'creative': ['art', 'design', 'music', 'writing', 'creative', 'media', 'storytelling'],\n",
        "            'social': ['teaching', 'counseling', 'social work', 'psychology', 'education', 'helping people'],\n",
        "            'research': ['research', 'analysis', 'investigation', 'study', 'academic'],\n",
        "            'communication': ['communication', 'public speaking', 'presentation', 'media', 'journalism']\n",
        "        }\n",
        "\n",
        "        for _, student in students_df.iterrows():\n",
        "            student_subjects = set(student['subjects'].split(', '))\n",
        "            student_interests = set(student['interests'].split(', '))\n",
        "\n",
        "            for _, course in courses_df.iterrows():\n",
        "                course_subjects = set(course['subjects'].split(', '))\n",
        "                course_skills = set(course['skills'].split(', '))\n",
        "\n",
        "                features = {}\n",
        "\n",
        "                # Student ID and Course ID (for reference)\n",
        "                features['student_id'] = student['student_id']\n",
        "                features['course_name'] = course['course_name']\n",
        "\n",
        "                # Basic subject matching features\n",
        "                for subject in all_subjects:\n",
        "                    features[f'student_has_{subject.lower()}'] = 1 if subject in student_subjects else 0\n",
        "                    features[f'course_requires_{subject.lower()}'] = 1 if subject in course_subjects else 0\n",
        "\n",
        "                # Subject overlap features\n",
        "                features['subject_overlap_count'] = len(student_subjects.intersection(course_subjects))\n",
        "                features['subject_overlap_ratio'] = (len(student_subjects.intersection(course_subjects)) /\n",
        "                                                   len(student_subjects.union(course_subjects)) if student_subjects.union(course_subjects) else 0)\n",
        "\n",
        "                # Interest-skill alignment features\n",
        "                features['interest_skill_overlap'] = len(student_interests.intersection(course_skills))\n",
        "                features['interest_skill_ratio'] = (len(student_interests.intersection(course_skills)) /\n",
        "                                                   len(student_interests.union(course_skills)) if student_interests.union(course_skills) else 0)\n",
        "\n",
        "                # Subject category features\n",
        "                student_stem_count = sum(1 for subj in student_subjects if subj in stem_subjects)\n",
        "                student_humanities_count = sum(1 for subj in student_subjects if subj in humanities_subjects)\n",
        "\n",
        "                course_stem_count = sum(1 for subj in course_subjects if subj in stem_subjects)\n",
        "                course_humanities_count = sum(1 for subj in course_subjects if subj in humanities_subjects)\n",
        "\n",
        "                features['student_stem_ratio'] = student_stem_count / len(stem_subjects)\n",
        "                features['student_humanities_ratio'] = student_humanities_count / len(humanities_subjects)\n",
        "                features['course_stem_ratio'] = course_stem_count / len(stem_subjects)\n",
        "                features['course_humanities_ratio'] = course_humanities_count / len(humanities_subjects)\n",
        "\n",
        "                # Alignment between student and course preferences\n",
        "                features['stem_alignment'] = min(features['student_stem_ratio'], features['course_stem_ratio'])\n",
        "                features['humanities_alignment'] = min(features['student_humanities_ratio'], features['course_humanities_ratio'])\n",
        "\n",
        "                # Interest category features\n",
        "                for category, keywords in interest_categories.items():\n",
        "                    student_category_score = sum(1 for interest in student_interests\n",
        "                                               if any(keyword.lower() in interest.lower() for keyword in keywords))\n",
        "                    course_category_score = sum(1 for skill in course_skills\n",
        "                                              if any(keyword.lower() in skill.lower() for keyword in keywords))\n",
        "\n",
        "                    features[f'student_{category}_interest'] = student_category_score\n",
        "                    features[f'course_{category}_relevance'] = course_category_score\n",
        "                    features[f'{category}_alignment'] = min(student_category_score, course_category_score)\n",
        "\n",
        "                # Diversity features\n",
        "                features['student_subject_diversity'] = len(student_subjects)\n",
        "                features['student_interest_diversity'] = len(student_interests)\n",
        "                features['course_subject_breadth'] = len(course_subjects)\n",
        "                features['course_skill_breadth'] = len(course_skills)\n",
        "\n",
        "                # Target variable: compatibility score\n",
        "                subject_score = len(student_subjects.intersection(course_subjects))\n",
        "                interest_score = len(student_interests.intersection(course_skills))\n",
        "                total_possible = len(student_subjects) + len(student_interests)\n",
        "\n",
        "                features['compatibility_score'] = (subject_score + interest_score) / total_possible if total_possible > 0 else 0\n",
        "\n",
        "                features_list.append(features)\n",
        "\n",
        "        return pd.DataFrame(features_list)\n",
        "\n",
        "    def prepare_training_data(self, students_df: pd.DataFrame, courses_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Prepare training data for XGBoost\n",
        "\n",
        "        Args:\n",
        "            students_df: DataFrame with student information\n",
        "            courses_df: DataFrame with course information\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (features, targets)\n",
        "        \"\"\"\n",
        "        logger.info(\"Extracting advanced features...\")\n",
        "\n",
        "        # Extract features\n",
        "        features_df = self.extract_advanced_features(students_df, courses_df)\n",
        "\n",
        "        # Separate features and target\n",
        "        target_col = 'compatibility_score'\n",
        "        feature_cols = [col for col in features_df.columns\n",
        "                       if col not in ['student_id', 'course_name', target_col]]\n",
        "\n",
        "        X = features_df[feature_cols]\n",
        "        y = features_df[target_col]\n",
        "\n",
        "        # Store feature names\n",
        "        self.feature_names = feature_cols\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        logger.info(f\"Prepared {X_scaled.shape[0]} training samples with {X_scaled.shape[1]} features\")\n",
        "\n",
        "        return X_scaled, y.values\n",
        "\n",
        "    def train(self, students_df: pd.DataFrame, courses_df: pd.DataFrame,\n",
        "              test_size=0.2, early_stopping_rounds=20):\n",
        "        \"\"\"\n",
        "        Train the XGBoost model\n",
        "\n",
        "        Args:\n",
        "            students_df: DataFrame with student information\n",
        "            courses_df: DataFrame with course information\n",
        "            test_size: Fraction of data for testing\n",
        "            early_stopping_rounds: Early stopping patience\n",
        "        \"\"\"\n",
        "        logger.info(\"Preparing training data...\")\n",
        "\n",
        "        # Prepare training data\n",
        "        X, y = self.prepare_training_data(students_df, courses_df)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=self.random_state, stratify=None\n",
        "        )\n",
        "\n",
        "        # Initialize XGBoost model\n",
        "        self.model = xgb.XGBRegressor(\n",
        "            n_estimators=self.n_estimators,\n",
        "            max_depth=self.max_depth,\n",
        "            learning_rate=self.learning_rate,\n",
        "            objective=self.objective,\n",
        "            random_state=self.random_state,\n",
        "            reg_alpha=0.1,  # L1 regularization\n",
        "            reg_lambda=0.1,  # L2 regularization\n",
        "            subsample=0.8,   # Subsample ratio\n",
        "            colsample_bytree=0.8,  # Feature sampling\n",
        "            eval_metric='rmse'\n",
        "        )\n",
        "\n",
        "        # Train model with early stopping\n",
        "        logger.info(\"Training XGBoost model...\")\n",
        "\n",
        "        self.model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_train, y_train), (X_test, y_test)],\n",
        "            early_stopping_rounds=early_stopping_rounds,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        # Evaluate model\n",
        "        train_pred = self.model.predict(X_train)\n",
        "        test_pred = self.model.predict(X_test)\n",
        "\n",
        "        train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
        "        test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
        "\n",
        "        logger.info(f\"Training RMSE: {train_rmse:.4f}\")\n",
        "        logger.info(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "\n",
        "        # Feature importance\n",
        "        feature_importance = self.model.feature_importances_\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': self.feature_names,\n",
        "            'importance': feature_importance\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        logger.info(\"Top 10 most important features:\")\n",
        "        for _, row in importance_df.head(10).iterrows():\n",
        "            logger.info(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "        logger.info(\"Training completed!\")\n",
        "\n",
        "        return {\n",
        "            'train_rmse': train_rmse,\n",
        "            'test_rmse': test_rmse,\n",
        "            'feature_importance': importance_df\n",
        "        }\n",
        "\n",
        "    def predict_for_student(self, student_data: Dict, courses_df: pd.DataFrame, top_k=5) -> List[Tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        Predict career recommendations for a single student\n",
        "\n",
        "        Args:\n",
        "            student_data: Dictionary with 'subjects' and 'interests' keys\n",
        "            courses_df: DataFrame with course information\n",
        "            top_k: Number of top recommendations to return\n",
        "\n",
        "        Returns:\n",
        "            List of (course_name, confidence_score) tuples\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "\n",
        "        # Create a temporary DataFrame for the student\n",
        "        temp_student_df = pd.DataFrame([{\n",
        "            'student_id': 'temp_student',\n",
        "            'subjects': ', '.join(student_data['subjects']),\n",
        "            'interests': ', '.join(student_data['interests'])\n",
        "        }])\n",
        "\n",
        "        # Extract features for all student-course combinations\n",
        "        features_df = self.extract_advanced_features(temp_student_df, courses_df)\n",
        "\n",
        "        # Prepare features for prediction\n",
        "        feature_cols = [col for col in features_df.columns\n",
        "                       if col not in ['student_id', 'course_name', 'compatibility_score']]\n",
        "\n",
        "        X = features_df[feature_cols]\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = self.model.predict(X_scaled)\n",
        "\n",
        "        # Combine with course names\n",
        "        recommendations = []\n",
        "        for i, course_name in enumerate(features_df['course_name']):\n",
        "            recommendations.append((course_name, float(predictions[i])))\n",
        "\n",
        "        # Sort by prediction score and return top K\n",
        "        recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return recommendations[:top_k]\n",
        "\n",
        "    def get_feature_importance(self, top_n=20) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Get feature importance from trained model\n",
        "\n",
        "        Args:\n",
        "            top_n: Number of top features to return\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with feature importance\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': self.feature_names,\n",
        "            'importance': self.model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        return importance_df.head(top_n)\n",
        "\n",
        "    def save_model(self, filepath: str):\n",
        "        \"\"\"Save the trained model and preprocessors\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"No model to save!\")\n",
        "\n",
        "        # Save XGBoost model\n",
        "        self.model.save_model(f\"{filepath}_xgboost.json\")\n",
        "\n",
        "        # Save preprocessors and metadata\n",
        "        with open(f\"{filepath}_preprocessors.pkl\", 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'scaler': self.scaler,\n",
        "                'course_encoder': self.course_encoder,\n",
        "                'feature_names': self.feature_names,\n",
        "                'n_estimators': self.n_estimators,\n",
        "                'max_depth': self.max_depth,\n",
        "                'learning_rate': self.learning_rate,\n",
        "                'objective': self.objective,\n",
        "                'random_state': self.random_state\n",
        "            }, f)\n",
        "\n",
        "        logger.info(f\"Model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath: str):\n",
        "        \"\"\"Load a trained model and preprocessors\"\"\"\n",
        "        # Load XGBoost model\n",
        "        self.model = xgb.XGBRegressor()\n",
        "        self.model.load_model(f\"{filepath}_xgboost.json\")\n",
        "\n",
        "        # Load preprocessors and metadata\n",
        "        with open(f\"{filepath}_preprocessors.pkl\", 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            self.scaler = data['scaler']\n",
        "            self.course_encoder = data['course_encoder']\n",
        "            self.feature_names = data['feature_names']\n",
        "            self.n_estimators = data['n_estimators']\n",
        "            self.max_depth = data['max_depth']\n",
        "            self.learning_rate = data['learning_rate']\n",
        "            self.objective = data['objective']\n",
        "            self.random_state = data['random_state']\n",
        "\n",
        "        logger.info(f\"Model loaded from {filepath}\")\n",
        "\n",
        "# Example usage and training script\n",
        "def main():\n",
        "    # Load data\n",
        "    students_df = pd.read_csv('./sample_data/student_data.csv')\n",
        "    courses_df = pd.read_csv('./sample_data/Courses.csv')\n",
        "\n",
        "    # Initialize recommender\n",
        "    recommender = XGBoostCareerRecommender(\n",
        "        n_estimators=200,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.1,\n",
        "        objective='reg:squarederror'\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    results = recommender.train(students_df, courses_df)\n",
        "\n",
        "    # Save model\n",
        "    recommender.save_model('xgboost_career_model')\n",
        "\n",
        "    # Display feature importance\n",
        "    importance_df = recommender.get_feature_importance()\n",
        "    print(\"\\nTop 15 Most Important Features:\")\n",
        "    print(importance_df.head(15))\n",
        "\n",
        "    # Test prediction\n",
        "    test_student = {\n",
        "        'subjects': ['Mathematics', 'Physics'],\n",
        "        'interests': ['Artificial Intelligence', 'Machine Learning']\n",
        "    }\n",
        "\n",
        "    recommendations = recommender.predict_for_student(test_student, courses_df)\n",
        "\n",
        "    print(\"\\nRecommendations for test student:\")\n",
        "    for course, confidence in recommendations:\n",
        "        print(f\"  {course}: {confidence:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "BmWXKK4V26RK",
        "outputId": "48344e24-45f8-40cf-f57e-360977b46aaf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 3 fields in line 171, saw 4\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-3355773337.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-10-3355773337.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m     \u001b[0mstudents_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./sample_data/student_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m     \u001b[0mcourses_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./sample_data/Courses.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 3 fields in line 171, saw 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBoe1-tE05iX",
        "outputId": "b4342a53-00e9-4631-9493-1e5df7f056bd"
      },
      "source": [
        "import xgboost as xgb\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, mean_squared_error\n",
        "import pickle\n",
        "import logging\n",
        "from typing import List, Dict, Tuple\n",
        "import itertools\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class XGBoostCareerRecommender:\n",
        "    def __init__(self, n_estimators=100, max_depth=6, learning_rate=0.1,\n",
        "                 objective='reg:squarederror', random_state=42):\n",
        "        \"\"\"\n",
        "        Initialize the XGBoost Career Recommender\n",
        "\n",
        "        Args:\n",
        "            n_estimators: Number of boosting rounds\n",
        "            max_depth: Maximum depth of trees\n",
        "            learning_rate: Learning rate\n",
        "            objective: XGBoost objective function\n",
        "            random_state: Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_depth = max_depth\n",
        "        self.learning_rate = learning_rate\n",
        "        self.objective = objective\n",
        "        self.random_state = random_state\n",
        "\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.course_encoder = LabelEncoder()\n",
        "        self.feature_names = []\n",
        "\n",
        "    def extract_advanced_features(self, students_df: pd.DataFrame, courses_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Extract comprehensive features for XGBoost training\n",
        "\n",
        "        Args:\n",
        "            students_df: DataFrame with student information\n",
        "            courses_df: DataFrame with course information\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with engineered features\n",
        "        \"\"\"\n",
        "        features_list = []\n",
        "\n",
        "        # Define subject categories\n",
        "        all_subjects = ['Mathematics', 'Physics', 'Chemistry', 'Biology', 'English', 'Geography', 'History']\n",
        "        stem_subjects = ['Mathematics', 'Physics', 'Chemistry', 'Biology']\n",
        "        humanities_subjects = ['English', 'Geography', 'History']\n",
        "\n",
        "        # Define interest categories\n",
        "        interest_categories = {\n",
        "            'technology': ['programming', 'computers', 'innovation', 'AI', 'software', 'data', 'machine learning', 'robotics'],\n",
        "            'healthcare': ['medicine', 'nursing', 'biology', 'helping', 'health', 'medical', 'care'],\n",
        "            'business': ['entrepreneurship', 'marketing', 'finance', 'management', 'economics', 'business'],\n",
        "            'creative': ['art', 'design', 'music', 'writing', 'creative', 'media', 'storytelling'],\n",
        "            'social': ['teaching', 'counseling', 'social work', 'psychology', 'education', 'helping people'],\n",
        "            'research': ['research', 'analysis', 'investigation', 'study', 'academic'],\n",
        "            'communication': ['communication', 'public speaking', 'presentation', 'media', 'journalism']\n",
        "        }\n",
        "\n",
        "        for index in students_df.index:\n",
        "            student = students_df.loc[index]\n",
        "\n",
        "            if 'subjects' not in student:\n",
        "                logger.error(f\"Error: 'subjects' not in student data for index {index}. Columns: {student.index.tolist()}\")\n",
        "                continue # Skip this student if 'subjects' is missing\n",
        "\n",
        "            student_subjects = set(student['subjects'].split(', '))\n",
        "            student_interests = set(student['interests'].split(', '))\n",
        "\n",
        "            for _, course in courses_df.iterrows():\n",
        "                course_subjects = set(course['subjects'].split(', '))\n",
        "                course_skills = set(course['skills'].split(', '))\n",
        "\n",
        "                features = {}\n",
        "\n",
        "                # Student ID and Course ID (for reference)\n",
        "                features['student_id'] = student['student_id']\n",
        "                features['course_name'] = course['course_name']\n",
        "\n",
        "                # Basic subject matching features\n",
        "                for subject in all_subjects:\n",
        "                    features[f'student_has_{subject.lower()}'] = 1 if subject in student_subjects else 0\n",
        "                    features[f'course_requires_{subject.lower()}'] = 1 if subject in course_subjects else 0\n",
        "\n",
        "                # Subject overlap features\n",
        "                features['subject_overlap_count'] = len(student_subjects.intersection(course_subjects))\n",
        "                features['subject_overlap_ratio'] = (len(student_subjects.intersection(course_subjects)) /\n",
        "                                                   len(student_subjects.union(course_subjects)) if student_subjects.union(course_subjects) else 0)\n",
        "\n",
        "                # Interest-skill alignment features\n",
        "                features['interest_skill_overlap'] = len(student_interests.intersection(course_skills))\n",
        "                features['interest_skill_ratio'] = (len(student_interests.intersection(course_skills)) /\n",
        "                                                   len(student_interests.union(course_skills)) if student_interests.union(course_skills) else 0)\n",
        "\n",
        "                # Subject category features\n",
        "                student_stem_count = sum(1 for subj in student_subjects if subj in stem_subjects)\n",
        "                student_humanities_count = sum(1 for subj in student_subjects if subj in humanities_subjects)\n",
        "\n",
        "                course_stem_count = sum(1 for subj in course_subjects if subj in stem_subjects)\n",
        "                course_humanities_count = sum(1 for subj in course_subjects if subj in humanities_subjects)\n",
        "\n",
        "                features['student_stem_ratio'] = student_stem_count / len(stem_subjects)\n",
        "                features['student_humanities_ratio'] = student_humanities_count / len(humanities_subjects)\n",
        "                features['course_stem_ratio'] = course_stem_count / len(stem_subjects)\n",
        "                features['course_humanities_ratio'] = course_humanities_count / len(humanities_subjects)\n",
        "\n",
        "                # Alignment between student and course preferences\n",
        "                features['stem_alignment'] = min(features['student_stem_ratio'], features['course_stem_ratio'])\n",
        "                features['humanities_alignment'] = min(features['student_humanities_ratio'], features['course_humanities_ratio'])\n",
        "\n",
        "                # Interest category features\n",
        "                for category, keywords in interest_categories.items():\n",
        "                    student_category_score = sum(1 for interest in student_interests\n",
        "                                               if any(keyword.lower() in interest.lower() for keyword in keywords))\n",
        "                    course_category_score = sum(1 for skill in course_skills\n",
        "                                              if any(keyword.lower() in skill.lower() for keyword in keywords))\n",
        "\n",
        "                    features[f'student_{category}_interest'] = student_category_score\n",
        "                    features[f'course_{category}_relevance'] = course_category_score\n",
        "                    features[f'{category}_alignment'] = min(student_category_score, course_category_score)\n",
        "\n",
        "                # Diversity features\n",
        "                features['student_subject_diversity'] = len(student_subjects)\n",
        "                features['student_interest_diversity'] = len(student_interests)\n",
        "                features['course_subject_breadth'] = len(course_subjects)\n",
        "                features['course_skill_breadth'] = len(course_skills)\n",
        "\n",
        "                # Target variable: compatibility score\n",
        "                subject_score = len(student_subjects.intersection(course_subjects))\n",
        "                interest_score = len(student_interests.intersection(course_skills))\n",
        "                total_possible = len(student_subjects) + len(student_interests)\n",
        "\n",
        "                features['compatibility_score'] = (subject_score + interest_score) / total_possible if total_possible > 0 else 0\n",
        "\n",
        "                features_list.append(features)\n",
        "\n",
        "        return pd.DataFrame(features_list)\n",
        "\n",
        "    def prepare_training_data(self, students_df: pd.DataFrame, courses_df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Prepare training data for XGBoost\n",
        "\n",
        "        Args:\n",
        "            students_df: DataFrame with student information\n",
        "            courses_df: DataFrame with course information\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (features, targets)\n",
        "        \"\"\"\n",
        "        logger.info(\"Extracting advanced features...\")\n",
        "\n",
        "        # Extract features\n",
        "        features_df = self.extract_advanced_features(students_df, courses_df)\n",
        "\n",
        "        # Separate features and target\n",
        "        target_col = 'compatibility_score'\n",
        "        feature_cols = [col for col in features_df.columns\n",
        "                       if col not in ['student_id', 'course_name', target_col]]\n",
        "\n",
        "        X = features_df[feature_cols]\n",
        "        y = features_df[target_col]\n",
        "\n",
        "        # Store feature names\n",
        "        self.feature_names = feature_cols\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "\n",
        "        logger.info(f\"Prepared {X_scaled.shape[0]} training samples with {X_scaled.shape[1]} features\")\n",
        "\n",
        "        return X_scaled, y.values\n",
        "\n",
        "    def train(self, students_df: pd.DataFrame, courses_df: pd.DataFrame,\n",
        "              test_size=0.2, early_stopping_rounds=20):\n",
        "        \"\"\"\n",
        "        Train the XGBoost model\n",
        "\n",
        "        Args:\n",
        "            students_df: DataFrame with student information\n",
        "            courses_df: DataFrame with course information\n",
        "            test_size: Fraction of data for testing\n",
        "            early_stopping_rounds: Early stopping patience\n",
        "        \"\"\"\n",
        "        logger.info(\"Preparing training data...\")\n",
        "\n",
        "        # Prepare training data\n",
        "        X, y = self.prepare_training_data(students_df, courses_df)\n",
        "\n",
        "        # Split data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=test_size, random_state=self.random_state, stratify=None\n",
        "        )\n",
        "\n",
        "        # Initialize XGBoost model\n",
        "        self.model = xgb.XGBRegressor(\n",
        "            n_estimators=self.n_estimators,\n",
        "            max_depth=self.max_depth,\n",
        "            learning_rate=self.learning_rate,\n",
        "            objective=self.objective,\n",
        "            random_state=self.random_state,\n",
        "            reg_alpha=0.1,  # L1 regularization\n",
        "            reg_lambda=0.1,  # L2 regularization\n",
        "            subsample=0.8,   # Subsample ratio\n",
        "            colsample_bytree=0.8,  # Feature sampling\n",
        "            eval_metric='rmse'\n",
        "        )\n",
        "\n",
        "        # Train model without early stopping callbacks to resolve TypeError\n",
        "        logger.info(\"Training XGBoost model...\")\n",
        "\n",
        "        self.model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_train, y_train), (X_test, y_test)],\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        # Evaluate model\n",
        "        train_pred = self.model.predict(X_train)\n",
        "        test_pred = self.model.predict(X_test)\n",
        "\n",
        "        train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
        "        test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))\n",
        "\n",
        "        logger.info(f\"Training RMSE: {train_rmse:.4f}\")\n",
        "        logger.info(f\"Test RMSE: {test_rmse:.4f}\")\n",
        "\n",
        "        # Feature importance\n",
        "        feature_importance = self.model.feature_importances_\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': self.feature_names,\n",
        "            'importance': feature_importance\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        logger.info(\"Top 10 most important features:\")\n",
        "        for _, row in importance_df.head(10).iterrows():\n",
        "            logger.info(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
        "\n",
        "        logger.info(\"Training completed!\")\n",
        "\n",
        "        return {\n",
        "            'train_rmse': train_rmse,\n",
        "            'test_rmse': test_rmse,\n",
        "            'feature_importance': importance_df\n",
        "        }\n",
        "\n",
        "    def predict_for_student(self, student_data: Dict, courses_df: pd.DataFrame, top_k=5) -> List[Tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        Predict career recommendations for a single student\n",
        "\n",
        "        Args:\n",
        "            student_data: Dictionary with 'subjects' and 'interests' keys\n",
        "            courses_df: DataFrame with course information\n",
        "            top_k: Number of top recommendations to return\n",
        "        Returns:\n",
        "            List of (course_name, confidence_score) tuples\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "\n",
        "        # Create a temporary DataFrame for the student\n",
        "        temp_student_df = pd.DataFrame([{\n",
        "            'student_id': 'temp_student',\n",
        "            'subjects': ', '.join(student_data['subjects']),\n",
        "            'interests': ', '.join(student_data['interests'])\n",
        "        }])\n",
        "\n",
        "        # Extract features for all student-course combinations\n",
        "        features_df = self.extract_advanced_features(temp_student_df, courses_df)\n",
        "\n",
        "        # Prepare features for prediction\n",
        "        feature_cols = [col for col in features_df.columns\n",
        "                       if col not in ['student_id', 'course_name', 'compatibility_score']]\n",
        "\n",
        "        X = features_df[feature_cols]\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = self.model.predict(X_scaled)\n",
        "\n",
        "        # Combine with course names\n",
        "        recommendations = []\n",
        "        for i, course_name in enumerate(features_df['course_name']):\n",
        "            recommendations.append((course_name, float(predictions[i])))\n",
        "\n",
        "        # Sort by prediction score and return top K\n",
        "        recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        return recommendations[:top_k]\n",
        "\n",
        "    def get_feature_importance(self, top_n=20) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Get feature importance from trained model\n",
        "\n",
        "        Args:\n",
        "            top_n: Number of top features to return\n",
        "\n",
        "        Returns:\n",
        "            DataFrame with feature importance\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet!\")\n",
        "\n",
        "        importance_df = pd.DataFrame({\n",
        "            'feature': self.feature_names,\n",
        "            'importance': self.model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        return importance_df.head(top_n)\n",
        "\n",
        "    def save_model(self, filepath: str):\n",
        "        \"\"\"Save the trained model and preprocessors\"\"\"\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"No model to save!\")\n",
        "\n",
        "        # Save XGBoost model\n",
        "        self.model.save_model(f\"{filepath}_xgboost.json\")\n",
        "\n",
        "        # Save preprocessors and metadata\n",
        "        with open(f\"{filepath}_preprocessors.pkl\", 'wb') as f:\n",
        "            pickle.dump({\n",
        "                'scaler': self.scaler,\n",
        "                'course_encoder': self.course_encoder,\n",
        "                'feature_names': self.feature_names,\n",
        "                'n_estimators': self.n_estimators,\n",
        "                'max_depth': self.max_depth,\n",
        "                'learning_rate': self.learning_rate,\n",
        "                'objective': self.objective,\n",
        "                'random_state': self.random_state\n",
        "            }, f)\n",
        "\n",
        "        logger.info(f\"Model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath: str):\n",
        "        \"\"\"Load a trained model and preprocessors\"\"\"\n",
        "        # Load XGBoost model\n",
        "        self.model = xgb.XGBRegressor()\n",
        "        self.model.load_model(f\"{filepath}_xgboost.json\")\n",
        "\n",
        "        # Load preprocessors and metadata\n",
        "        with open(f\"{filepath}_preprocessors.pkl\", 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "            self.scaler = data['scaler']\n",
        "            self.course_encoder = data['course_encoder']\n",
        "            self.feature_names = data['feature_names']\n",
        "            self.n_estimators = data['n_estimators']\n",
        "            self.max_depth = data['max_depth']\n",
        "            self.learning_rate = data['learning_rate']\n",
        "            self.objective = data['objective']\n",
        "            self.random_state = data['random_state']\n",
        "\n",
        "        logger.info(f\"Model loaded from {filepath}\")\n",
        "\n",
        "# Example usage and training script\n",
        "def main():\n",
        "    # Load data\n",
        "    students_df = pd.read_csv('./sample_data/student_data.csv', on_bad_lines='skip')\n",
        "    courses_df = pd.read_csv('./sample_data/Courses.csv')\n",
        "\n",
        "    print(\"Columns in students_df:\", students_df.columns)\n",
        "    print(\"Head of students_df:\\n\", students_df.head())\n",
        "    print(\"Columns in courses_df:\", courses_df.columns)\n",
        "\n",
        "    # Debugging: Access 'subjects' column for the first row\n",
        "    if not students_df.empty:\n",
        "        try:\n",
        "            first_subject = students_df.loc[0, 'subjects']\n",
        "            print(f\"Successfully accessed 'subjects' for first row: {first_subject}\")\n",
        "        except KeyError as e:\n",
        "            print(f\"KeyError when accessing 'subjects' for first row: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred when accessing 'subjects' for first row: {e}\")\n",
        "\n",
        "\n",
        "    # Initialize recommender\n",
        "    recommender = XGBoostCareerRecommender(\n",
        "        n_estimators=200,\n",
        "        max_depth=8,\n",
        "        learning_rate=0.1,\n",
        "        objective='reg:squarederror'\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    results = recommender.train(students_df, courses_df)\n",
        "\n",
        "    # Save model\n",
        "    recommender.save_model('xgboost_career_model')\n",
        "\n",
        "    # Display feature importance\n",
        "    importance_df = recommender.get_feature_importance()\n",
        "    print(\"\\nTop 15 Most Important Features:\")\n",
        "    print(importance_df.head(15))\n",
        "\n",
        "    # Test prediction\n",
        "    test_student = {\n",
        "        'subjects': ['Mathematics', 'Physics'],\n",
        "        'interests': ['Artificial Intelligence', 'Machine Learning']\n",
        "    }\n",
        "\n",
        "    recommendations = recommender.predict_for_student(test_student, courses_df)\n",
        "\n",
        "    print(\"\\nRecommendations for test student:\")\n",
        "    for course, confidence in recommendations:\n",
        "        print(f\"  {course}: {confidence:.3f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in students_df: Index(['student_id', 'subjects', 'interests'], dtype='object')\n",
            "Head of students_df:\n",
            "   student_id                                subjects  \\\n",
            "0       S001  Mathematics, Physics, Computer Studies   \n",
            "1       S002             English, Kiswahili, History   \n",
            "2       S003         Biology, Chemistry, Agriculture   \n",
            "3       S004    Geography, History, Business Studies   \n",
            "4       S005          Art and Design, Music, English   \n",
            "\n",
            "                                    interests  \n",
            "0           Robotics, Artificial Intelligence  \n",
            "1                    Creative Writing, Poetry  \n",
            "2  Environmental Science, Sustainable Farming  \n",
            "3                   Urban Planning, Economics  \n",
            "4                 Graphic Design, Songwriting  \n",
            "Columns in courses_df: Index(['course_id', 'course_name', 'subjects', 'skills'], dtype='object')\n",
            "Successfully accessed 'subjects' for first row: Mathematics, Physics, Computer Studies\n",
            "\n",
            "Top 15 Most Important Features:\n",
            "                       feature  importance\n",
            "14       subject_overlap_count    0.901424\n",
            "15       subject_overlap_ratio    0.075985\n",
            "45   student_subject_diversity    0.008156\n",
            "22              stem_alignment    0.004043\n",
            "46  student_interest_diversity    0.002899\n",
            "16      interest_skill_overlap    0.001582\n",
            "17        interest_skill_ratio    0.000943\n",
            "18          student_stem_ratio    0.000761\n",
            "3      course_requires_physics    0.000586\n",
            "47      course_subject_breadth    0.000512\n",
            "9      course_requires_english    0.000499\n",
            "2          student_has_physics    0.000445\n",
            "6          student_has_biology    0.000380\n",
            "4        student_has_chemistry    0.000285\n",
            "20           course_stem_ratio    0.000157\n",
            "\n",
            "Recommendations for test student:\n",
            "  Bachelor of Science (Geospatial Information Science With IT): 0.437\n",
            "  Bachelor of Technology (Geoinformation Technology): 0.437\n",
            "  Bachelor of Science (Computer Science): 0.437\n",
            "  Bachelor of Science (Mathematics & Computer Science): 0.437\n",
            "  Bachelor of Science in Computer Science: 0.437\n"
          ]
        }
      ]
    }
  ]
}